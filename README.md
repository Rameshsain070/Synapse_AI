# ğŸš€ SynapseAI â€“ Modular Agentic AI Platform

A production-ready, modular AI agent platform built using **FastAPI, LangGraph, Azure OpenAI, and Pinecone**.

SynapseAI combines conversational AI agents, Retrieval-Augmented Generation (RAG), long-term semantic memory, and streaming LLM responses into a scalable, containerized backend system.

This project demonstrates end-to-end AI system design â€” from backend architecture and vector databases to deployment, monitoring, and evaluation.

---

## ğŸ“Œ Project Overview

SynapseAI was built to provide a scalable foundation for developing AI-powered applications with:

- ğŸ¤– Conversational AI agents  
- ğŸ“š Retrieval-Augmented Generation (RAG)  
- ğŸ§  Long-term semantic memory  
- ğŸ” Secure authentication  
- âš¡ Real-time streaming responses  
- ğŸ“Š Observability and monitoring  
- ğŸ³ Containerized production deployment  

The system follows **service-oriented architecture principles**, enabling modular scaling and clean separation of concerns.

---

## ğŸ—ï¸ System Architecture

```
User
  â†“
FastAPI Backend
  â†“
LangGraph Agent
  â†“
Azure OpenAI (LLM)
  â†“
Vector Database (Pinecone / pgvector)
  â†“
PostgreSQL (User & Session Data)
```

---

## ğŸ§  Core Components

### 1ï¸âƒ£ AI Agent Workflow (LangGraph)

- State-driven agent execution  
- Tool calling and function execution  
- Conversational memory handling  
- Event-based flow control  
- Streaming and non-streaming responses  

LangGraph was used to maintain structured reasoning flow and state persistence.

---

### 2ï¸âƒ£ Retrieval-Augmented Generation (RAG)

The RAG pipeline was implemented as follows:

1. Documents were ingested and split into chunks using `RecursiveCharacterTextSplitter`.
2. Chunks were converted into embeddings using OpenAI embedding models.
3. Embeddings were stored in **Pinecone**.
4. On user query:
   - Query was embedded.
   - Semantic similarity search retrieved relevant chunks.
   - Retrieved context was passed to the LLM.
   - A context-aware response was generated.

This significantly improved response accuracy and reduced hallucinations.

---

### 3ï¸âƒ£ Long-Term Memory System

The long-term memory system was powered by:

- **pgvector** for semantic similarity search  
- **PostgreSQL** for structured storage  
- User-isolated memory collections  

Capabilities:

- Automatic memory extraction  
- Context-aware retrieval  
- Dynamic memory updates  

---

### 4ï¸âƒ£ FastAPI Backend

The backend included:

- JWT-based authentication  
- Session management  
- Rate limiting  
- CORS configuration  
- Structured logging  
- Streaming LLM responses (NDJSON)  

API endpoints included:

- `/api/v1/auth/*`
- `/api/v1/chatbot/chat`
- `/api/v1/chatbot/chat/stream`
- `/health`
- `/metrics`

Swagger documentation available at:

```
http://localhost:8000/docs
```

---

### 5ï¸âƒ£ LLM Integration

Integrated with Azure OpenAI models:

- GPT-4o  
- GPT-4o-mini  
- GPT-5 variants  

Features:

- Automatic retry logic (exponential backoff)  
- Configurable temperature & token limits  
- Environment-specific configurations  
- Fallback handling  

---

### 6ï¸âƒ£ Observability & Monitoring

Monitoring stack included:

- ğŸ“ˆ Prometheus (metrics collection)  
- ğŸ“Š Grafana (dashboard visualization)  
- ğŸ” Langfuse (LLM tracing)  
- Structured logging with request context  

Tracked metrics:

- API latency  
- Rate limiting stats  
- Database performance  
- Token usage  
- System resource utilization  

---

### 7ï¸âƒ£ Model Evaluation Framework

An automated evaluation pipeline was implemented to measure LLM performance.

Features:

- Trace-based evaluation  
- Metric scoring  
- JSON report generation  
- Success/failure analysis  

Reports generated in:

```
evals/reports/
```

---

## ğŸ› ï¸ Tech Stack

### ğŸ”¹ Backend
- Python  
- FastAPI  
- LangGraph  
- LangChain  

### ğŸ”¹ AI & Data
- Azure OpenAI  
- OpenAI Embeddings  
- Pinecone  
- PostgreSQL  
- pgvector  

### ğŸ”¹ DevOps
- Docker  
- Docker Compose  
- Azure Container Apps  

### ğŸ”¹ Monitoring
- Prometheus  
- Grafana  
- Langfuse  

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd synapseai
```

---

### 2ï¸âƒ£ Create Environment File

Create a `.env` file:

```env
APP_ENV=development
OPENAI_API_KEY=your_openai_key
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=mydb
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
LANGFUSE_PUBLIC_KEY=your_key
LANGFUSE_SECRET_KEY=your_secret
SECRET_KEY=your_jwt_secret
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
uv sync
source .venv/bin/activate
```

---

### 4ï¸âƒ£ Run Locally

```bash
make dev
```

Access Swagger UI:

```
http://localhost:8000/docs
```

---

## ğŸ³ Running with Docker

```bash
docker compose up --build
```

This launches:

- FastAPI backend  
- PostgreSQL  
- Prometheus  
- Grafana  

Monitoring URLs:

Prometheus:
```
http://localhost:9090
```

Grafana:
```
http://localhost:3000
```

Default credentials:

```
Username: admin
Password: admin
```

---

## ğŸ“Š Running Model Evaluation

```bash
make eval
```

Reports generated in:

```
evals/reports/
```

---

## ğŸ” Security Features

- JWT authentication  
- Token expiration handling  
- Rate limiting  
- Input validation  
- CORS restrictions  
- Structured request logging  

---

## ğŸ”® Future Enhancements

- Multi-agent orchestration  
- Redis caching  
- Kubernetes deployment  
- CI/CD automation  
- Frontend dashboard  
- Horizontal scaling  

---

## ğŸ“Œ Key Learnings

This project involved:

- Designing modular AI architecture  
- Implementing production-grade backend systems  
- Building RAG pipelines  
- Integrating vector databases  
- Deploying containerized services  
- Monitoring LLM systems in production  
- Evaluating AI system performance  

---

## ğŸ‘¨â€ğŸ’» Author

**Ramesh Sain**  
M.Tech CSE â€“ Artificial Intelligence & Machine Learning  

---
